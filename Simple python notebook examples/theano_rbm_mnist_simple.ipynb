{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A simple one hidden layer neural net example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import py_compile\n",
    "#py_compile.compile('rbm_classic.py')\n",
    "\n",
    "\n",
    "print \"Starting program\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of mnist data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the needed modules and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import needed modules\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "import os\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "from utils import tile_raster_images\n",
    "\n",
    "print \"Looking for mnist data\"\n",
    "\n",
    "#############\n",
    "# LOAD DATA #\n",
    "#############\n",
    "dataset = 'mnist.pkl.gz' \n",
    "\n",
    "# Download the MNIST dataset if it is not present\n",
    "data_dir, data_file = os.path.split(dataset) \n",
    "\n",
    "# If the file does not exist and there was no data_dir specified\n",
    "if not os.path.isfile(dataset) and data_dir == \"\":\n",
    "    print \"File %s not found\" %dataset\n",
    "    print \"Looking in data directory\"\n",
    "    # Check if dataset is in the data directory.\n",
    "    new_path = os.path.join(\n",
    "        os.getcwd(),\n",
    "        \"Python\",\n",
    "        \"data\",\n",
    "        \"mnist\",\n",
    "        dataset\n",
    "    )\n",
    "    dataset = new_path\n",
    "\n",
    "# If the dataset had a data_dir or we added the standard\n",
    "# data directory, look for the file there\n",
    "if not os.path.isfile(dataset):\n",
    "    print \"File %s not found\" %dataset\n",
    "    print \"Downloading mnist file from web\"\n",
    "    # If the file is still not found, \n",
    "    # we download a copy\n",
    "    import urllib\n",
    "    origin = (\n",
    "        'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "    )\n",
    "    print 'Downloading data from %s' % origin\n",
    "    urllib.urlretrieve(origin, dataset)\n",
    "else:\n",
    "    # If we find the file, we notify the user\n",
    "    print \"File %s found\" %dataset\n",
    "\n",
    "print '... loading data'\n",
    "# Load the dataset\n",
    "f = gzip.open(dataset, 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "print '... data loaded'\n",
    "\n",
    "#train_set, valid_set, test_set format: tuple(input, target)\n",
    "#input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "#witch row's correspond to an example. target is a\n",
    "#numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "#the number of rows in the input. It should give the target\n",
    "#target to the example with the same index in the input.\n",
    "\n",
    "\n",
    "start_training   = 0\n",
    "start_validation = 0\n",
    "start_testing    = 0\n",
    "\n",
    "train_set_x, train_set_y = train_set\n",
    "\n",
    "valid_set_x, valid_set_y = valid_set\n",
    "\n",
    "test_set_x,  test_set_y  = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of mnist or other data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign names to the different sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = (train_set_x)\n",
    "\n",
    "Valid = (valid_set_x)\n",
    "Test =  (test_set_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training rate and number of training steps.\n",
    "The training rate defines how fast we move towards\n",
    "minimising the error (too large a training rate can make\n",
    "the algorithm overshoot the minimum and never achieve stability).\n",
    "The number of training steps define after how many steps we \n",
    "stop training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training rate\n",
    "#tr_rate = 0.01\n",
    "# number of epochs\n",
    "#epochs = 10\n",
    "# number of neurons in hidden layer\n",
    "\n",
    "neuron = 10\n",
    "nhidden = neuron*neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the symbolic variables.\n",
    "x will represent the input, i.e. a matrix of random numbers of size feats for each example (this is the D[0] entry defined above.\n",
    "y will represent the output, i.e. whether the example belongs to class 0 or class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats = Train.shape[1]\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the vector of weights and the bias.\n",
    "There are num_classes weights associated to each feature, where the output is num_classes neurons.\n",
    "There are num_classes biases since there are num_classes output neurons.\n",
    "The weights are randomly initialised, the biases can be initialised to 0.0 or a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights of the network\n",
    "w = theano.shared(rng.normal(0, 1, (feats, nhidden)), borrow = True)\n",
    "\n",
    "# Biases for input to hidden layer\n",
    "bh = theano.shared(numpy.full(nhidden, 0.01), borrow = True)\n",
    "\n",
    "# The biases from hidden to output layer (same as input)\n",
    "by = theano.shared(numpy.full(feats, 0.01), borrow = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the actual solution. \n",
    "Sigma represent the sigmoid \n",
    "<br>\n",
    "$$ \\frac{1}{1+exp(-\\bf{x}\\dot\\bf{w}-\\bf{b}) } $$ \n",
    "<br>\n",
    "\n",
    "that is expressed in theano as T.nnet.sigmoid().\n",
    "\n",
    "To check the result, it is enough to check whether sigma is greater than 0.5, otherwise we take the class with the highest output (probability). \n",
    "The prediction then can either be 0 or 1 (correct or false) depending on whether the sigmoid is greater or less than 0.5.\n",
    "The cost function is defined by \n",
    "\n",
    "<br>\n",
    "$$ \n",
    "error({\\bf w}) = -\\frac{1}{N} \\sum_{i=1}^{N} [ y^i \\ln (\\sigma({\\bf{x^i}})) + (1-y^i) \\ln (1 - \\sigma({\\bf{x^i}})] \n",
    "$$\n",
    "<br>\n",
    "\n",
    "where the superscript represents the $i^{th}$ example.\n",
    "\n",
    "The cost adds a value to reduce the possibility of overfitting by keeping larger weights in check.\n",
    "Finally, theano will calculate the gradient of the cost function that is used for approximating the solution using linear descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_examples = 100\n",
    "tr_rate = 1.0/training_examples\n",
    "\n",
    "x_states = x > numpy.random.rand(training_examples, feats)\n",
    "\n",
    "hid =  T.nnet.sigmoid(T.dot(x, w) + bh)    # Activation function of hidden layer\n",
    "hid_states = hid > numpy.random.rand(training_examples, nhidden)\n",
    "\n",
    "# Construct Theano expression graph\n",
    "vis = T.nnet.sigmoid(T.dot(hid_states, w.T) + by) \n",
    "vis_states = vis > numpy.random.rand(training_examples, feats)\n",
    "\n",
    "hid2 = T.nnet.sigmoid(T.dot(vis, w) + bh)\n",
    "hid2_states = hid2 > numpy.random.rand(training_examples, nhidden)\n",
    "\n",
    "hid_par = T.nnet.sigmoid(T.dot(x, w) + bh)\n",
    "vis_par = T.nnet.sigmoid(T.dot(hid_par, w.T) + by)\n",
    "hid2_par = hid2 = T.nnet.sigmoid(T.dot(vis_par, w) + bh)\n",
    "\n",
    "prediction = vis\n",
    "\n",
    "# Cross-entropy loss function\n",
    "#xent = -x * T.log(vis) - (1-x) * T.log(1-vis)\n",
    "xent = T.sum((x - vis)**2)\n",
    "parameters = [w, bh, by]  # this line defines all parameters for each layer\n",
    "\n",
    "cost = xent.mean()\n",
    "\n",
    "#diag = numpy.asmatrix(-T.dot(T.dot(x, w), hid.T))\n",
    "energy = -(-T.dot(T.dot(x, w), hid.T) - T.dot(x,by) - T.dot(hid,bh)).mean()\n",
    "#cost = xent.mean()\n",
    "\n",
    "pos_associations = T.dot(x.T, hid)\n",
    "neg_associations = T.dot(vis_par.T, hid2)\n",
    "\n",
    "# Compute the gradient of the cost  \n",
    "gparameters = [T.grad(cost, param) for param in parameters]    \n",
    "\n",
    "wparameters  = [w]\n",
    "byparameters = [by]\n",
    "bhparameters = [bh]\n",
    "\n",
    "\n",
    "update = []\n",
    "for wparam, byparam, bhparam in zip(wparameters, byparameters, bhparameters):\n",
    "    update.append((wparam, wparam + tr_rate*(pos_associations - neg_associations)))\n",
    "    update.append((byparam, byparam + tr_rate*(x.T.sum(axis=1) - vis.T.sum(axis=1))))      \n",
    "    update.append((bhparam, bhparam + tr_rate*(hid.T.sum(axis=1) - hid2.T.sum(axis=1))))\n",
    "    \n",
    "#update = [(param, param - tr_rate * gparam) for param, gparam in zip(parameters, gparameters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We create the theano function.\n",
    "The input is given by the set of features per each example.\n",
    "The output is given by the class per each example.\n",
    "The training is performed by updating weights and biases using the gradient calculated times a training rate (in order to avoid overshooting the minimum value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def free_energy(v_sample):\n",
    "    ''' Function to compute the free energy '''\n",
    "    wx_b = T.dot(v_sample, w) + bh\n",
    "    vbias_term = T.dot(v_sample, by)\n",
    "    hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "    return -hidden_term - vbias_term\n",
    "\n",
    "#energy = free_energy(x).sum()\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x],\n",
    "          outputs=[cost],\n",
    "          updates=update)\n",
    "\n",
    "predict = theano.function(inputs=[x], outputs=[prediction])\n",
    "\n",
    "hiddenImg = theano.function(\n",
    "              inputs=[x],\n",
    "              outputs=[hid] )\n",
    "\n",
    "energyVal = theano.function(\n",
    "            inputs=[x],\n",
    "            outputs=[energy] )\n",
    "\n",
    "debug = theano.function(\n",
    "            inputs=[x],\n",
    "            outputs = [pos_associations, neg_associations, pos_associations-neg_associations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the actual training on the data. \n",
    "This updates at each step the weighs and bias making the neural net perform better and get closer to the target solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SGD(set):\n",
    "    # If we don't want to load all the data,\n",
    "    # we can just load a subset with a small\n",
    "    # number of examples.\n",
    "    # Set it to zero or a negative number\n",
    "    # if you want to load the whole data set\n",
    "    #training_examples = 100\n",
    "\n",
    "    # You want to always\n",
    "    # load the same examples or randomise\n",
    "    # the subset\n",
    "    \n",
    "    start_training   = numpy.random.randint(set.shape[0] - training_examples)\n",
    "\n",
    "    Train = set[start_training:training_examples + start_training]\n",
    "    \n",
    "    return Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "step = 1000\n",
    "epochs = 10*step\n",
    "\n",
    "# Train\n",
    "start_cpu_time = time.clock()\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    Train = SGD(train_set[0])\n",
    "    if epoch % step == 0:\n",
    "        print \"Epoch:\", epoch\n",
    "        tmp = time.clock()\n",
    "        tmp2 = time.time()\n",
    "    \n",
    "    #Do 100 epochs on the small subset\n",
    "    for count in range(1):\n",
    "        otp = train(Train)\n",
    "        \n",
    "    if epoch % step == 0:   \n",
    "        #print \"energy = \", numpy.asarray(energyVal(Train))\n",
    "        print \"cpu time = \", time.clock() - tmp\n",
    "        print \"wall clock time = \", time.time() - tmp2\n",
    "        print \"cost = \", numpy.asarray(otp)\n",
    "    \n",
    "    if epoch % step == 0:\n",
    "        # Construct image from the weight matrix\n",
    "        image = Image.fromarray(\n",
    "            tile_raster_images(\n",
    "            w.get_value(borrow=True).T,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(neuron, neuron),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "        )\n",
    "        image.show()\n",
    "    #print \"debug = \", numpy.asarray(debug(Train))\n",
    "end_cpu_time = time.clock()\n",
    "end_time = time.time()\n",
    "print \"Average cpu time per epoch=\", (end_cpu_time-start_cpu_time)/epochs\n",
    "print \"Average wall clock time per epoch=\", (end_time-start_time)/epochs\n",
    "print \"Total cpu time = \", end_cpu_time - start_cpu_time\n",
    "print \"Total time = \", end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the errors, i.e. the numbers of examples in the training set that have not been classified correctly and output the accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0\n",
    "\n",
    "print \"Results for Training set\"\n",
    "\n",
    "N = Train.shape[0]\n",
    "error = 0\n",
    "\n",
    "if N > 1000 : draw_images = False\n",
    "result = predict(Train)\n",
    "\n",
    "N = 5\n",
    "#for index in range(N):\n",
    "for index in range(N):  \n",
    "    if True :\n",
    "        tmp = numpy.reshape(Train[index], [28, 28])\n",
    "        plt.figure()\n",
    "        plt.imshow(tmp, cmap = cm.Greys_r)\n",
    "\n",
    "# hiddenImg are the values of the neurons in the hidden layer\n",
    "tmp2 = numpy.asarray(hiddenImg(Train))\n",
    "\n",
    "t = T.dvector('t')\n",
    "threshold = 0.001  # change accordingly\n",
    "t_clipped = t * (t > threshold)\n",
    "\n",
    "f = theano.function(inputs=[t], outputs=t_clipped)\n",
    "\n",
    "#for img in range(tmp.shape[1]) :  \n",
    "for img in range(0) :  \n",
    "    plt.figure()\n",
    "    plt.imshow(numpy.reshape(tmp2[0][img], [20, 20]), cmap = cm.Greys_r)\n",
    "    \n",
    "# predict values are the values of the neurons in the reconstructed layer\n",
    "tmp3 = numpy.asarray(predict(Train))\n",
    "\n",
    "#for img in range(tmp.shape[1]) :  \n",
    "for img in range(N) :  \n",
    "    plt.figure()\n",
    "    plt.imshow(numpy.reshape(tmp3[0][img], [28, 28]), cmap = cm.Greys_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "from utils import tile_raster_images\n",
    "\n",
    "print w.T.shape.eval()\n",
    "print predict(Train)[0].shape\n",
    "print Train.shape\n",
    "\n",
    "def show_weights():\n",
    "    # Construct image from the weight matrix\n",
    "    image = Image.fromarray(\n",
    "        tile_raster_images(\n",
    "            w.get_value(borrow=True).T,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(neuron, neuron),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "    )\n",
    "    image.show()\n",
    "\n",
    "def show_reconstruction(): \n",
    "    image = Image.fromarray(\n",
    "        tile_raster_images(\n",
    "            predict(Train)[0],\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(10, 10),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "    )\n",
    "    image.show()\n",
    "\n",
    "def show_orig():\n",
    "    image = Image.fromarray(\n",
    "        tile_raster_images(\n",
    "            Train,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(10, 10),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "    )\n",
    "    image.show()\n",
    "\n",
    "image.save('filters_at_epoch_.png')\n",
    "Image.open('filters_at_epoch_.png')\n",
    "\n",
    "show_orig()\n",
    "show_reconstruction()\n",
    "show_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = theano.shared(rng.normal(0, 1, 2))\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "z = theano.shared(rng.normal(0, 1, (2, 2)))\n",
    "print v.eval()\n",
    "print z.eval()\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "print z[:][0].eval()\n",
    "print T.sum(z.T).eval()\n",
    "print T.sum(z.T[:][0]).eval()\n",
    "\n",
    "print numpy.sum([])\n",
    "print numpy.sum(z[:][0].eval())\n",
    "print v[0].eval()\n",
    "\n",
    "y = v[0] + T.sum(z[:][0])\n",
    "\n",
    "y = v + z.sum(axis=1)\n",
    "\n",
    "print y.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = SGD(valid_set[0])\n",
    "\n",
    "print test_set[0].shape[0]\n",
    "print Train.shape\n",
    "print predict(Train)[0].shape\n",
    "\n",
    "\n",
    "show_orig()\n",
    "show_reconstruction()\n",
    "show_weights()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
