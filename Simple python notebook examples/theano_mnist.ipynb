{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple logistic regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Starting program\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what kind of data we are loading and predicting.\n",
    "Data can be:\n",
    "1) random, in which case we define a random array of values\n",
    "and output classes\n",
    "2) mnist, in which case we use the mnist dataset\n",
    "3) other, in which case we define the dataset we want to use\n",
    "\n",
    "If data is mnist we will draw the numbers the program misclassified, if we limit the dataset to a 1000 examples or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data can be random, or other (in particular if other is not specified it will be mnist)\n",
    "data = \"random\"\n",
    "\n",
    "if data == \"mnist\" : draw_images = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of random data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is random:\n",
    "\n",
    "Import needed modules,\n",
    "\n",
    "Define the number of input neurons (the features) and the number of examples,\n",
    "\n",
    "Define the number of output classes\n",
    "\n",
    "Create the data--\n",
    "    for the data, define a tensor with two entries: \n",
    "    the first entry is a matrix of size N (number of examples) by feats (number of features) \n",
    "    of random numbers on a normal distribution around 0. \n",
    "    The second entry is a vector of size N (number of examples) of either 0 or 1 (the two classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data == \"random\":\n",
    "    \n",
    "    # Define the number of input neurons (the features) and the number of examples,\n",
    "    # N = number of examples\n",
    "    N = 200\n",
    "    #feats = number of input neurons\n",
    "    feats = 784 \n",
    "\n",
    "    # Define the number of output classes\n",
    "    num_classes = 10\n",
    "        \n",
    "    # Create the data\n",
    "    D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=num_classes)) \n",
    "    train_set_x = D[0] \n",
    "    train_set_y = D[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data, define a tensor with two entries: the first entry is a matrix of size N (number of examples) by feats (number of features) of random numbers on a normal distribution around 0. The second entry is a vector of size N (number of examples) of either 0 or 1 (the two classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of random data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of mnist or other data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Else, import the needed modules and load the data.\n",
    "\n",
    "Split the data in input and output (the target results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data != \"random\":\n",
    "\n",
    "    # Import needed modules\n",
    "    import numpy\n",
    "    import theano\n",
    "    import theano.tensor as T\n",
    "    rng = numpy.random\n",
    "    import os\n",
    "    import gzip\n",
    "    import cPickle\n",
    "\n",
    "    print \"Looking for mnist data\"\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "    if data == \"mnist\":\n",
    "        dataset = 'mnist.pkl.gz' \n",
    "\n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset) \n",
    "\n",
    "    # If the file does not exist and there was no data_dir specified\n",
    "    if not os.path.isfile(dataset) and data_dir == \"\":\n",
    "        print \"File %s not found\" %dataset\n",
    "        print \"Looking in data directory\"\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.getcwd(),\n",
    "            \"Python\",\n",
    "            \"data\",\n",
    "            \"mnist\",\n",
    "            dataset\n",
    "        )\n",
    "        dataset = new_path\n",
    "\n",
    "    # If the dataset had a data_dir or we added the standard\n",
    "    # data directory, look for the file there\n",
    "    if not os.path.isfile(dataset):\n",
    "        print \"File %s not found\" %dataset\n",
    "        print \"Downloading mnist file from web\"\n",
    "        # If the file is still not found, \n",
    "        # we download a copy\n",
    "        import urllib\n",
    "        origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        )\n",
    "        print 'Downloading data from %s' % origin\n",
    "        urllib.urlretrieve(origin, dataset)\n",
    "    else:\n",
    "        # If we find the file, we notify the user\n",
    "        print \"File %s found\" %dataset\n",
    "\n",
    "    print '... loading data'\n",
    "    # Load the dataset\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    print '... data loaded'\n",
    "\n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #witch row's correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    "    \n",
    "    # If we don't want to load all the data,\n",
    "    # we can just load a subset with a small\n",
    "    # number of examples.\n",
    "    # Set it to zero or a negative number\n",
    "    # if you want to load the whole data set\n",
    "    training_examples = 100\n",
    "    validation_examples = 20\n",
    "    testing_examples = 20\n",
    "    \n",
    "    # This is whether you want to always\n",
    "    # load the same examples or randomise\n",
    "    # the subset\n",
    "    random_examples = True\n",
    "    \n",
    "    if(random_examples):\n",
    "        start_training   = numpy.random.randint(50000 - training_examples)\n",
    "        start_validation = numpy.random.randint(10000 - validation_examples)\n",
    "        start_testing    = numpy.random.randint(10000 - testing_examples)\n",
    "    else:\n",
    "        start_training   = 0\n",
    "        start_validation = 0\n",
    "        start_testing    = 0\n",
    "\n",
    "    if(training_examples <= 0):\n",
    "        train_set_x, train_set_y = train_set\n",
    "    else:\n",
    "        train_set_x = train_set[0][start_training:training_examples + start_training]\n",
    "        train_set_y = train_set[1][start_training:training_examples + start_training]\n",
    "        \n",
    "    if(validation_examples <=0):\n",
    "        valid_set_x, valid_set_y = valid_set\n",
    "    else:\n",
    "        valid_set_x = valid_set[0][start_validation:validation_examples + start_validation]\n",
    "        valid_set_y = valid_set[1][start_validation:validation_examples + start_validation]\n",
    "        \n",
    "    if(testing_examples <= 0):\n",
    "        test_set_x,  test_set_y  = test_set\n",
    "    else:\n",
    "        test_set_x  = test_set [0][start_testing:testing_examples + start_testing]\n",
    "        test_set_y  = test_set [1][start_testing:testing_examples + start_testing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of mnist or other data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign names to the different sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = (train_set_x, train_set_y)\n",
    "\n",
    "if data == \"mnist\":\n",
    "    Valid = (valid_set_x, valid_set_y)\n",
    "    Test =  (test_set_x,  test_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training rate and number of training steps.\n",
    "The training rate defines how fast we move towards\n",
    "minimising the error (too large a training rate can make\n",
    "the algorithm overshoot the minimum and never achieve stability).\n",
    "The number of training steps define after how many steps we \n",
    "stop training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training rate\n",
    "tr_rate = 0.1\n",
    "\n",
    "training_steps = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the symbolic variables.\n",
    "x will represent the input, i.e. a matrix of random numbers of size feats for each example (this is the D[0] entry defined above.\n",
    "y will represent the output, i.e. whether the example belongs to class 0 or class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats = Train[0].shape[1]\n",
    "num_classes = numpy.amax(train_set_y)-numpy.amin(train_set_y)+1\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "if(num_classes< 3):\n",
    "    y = T.vector(\"y\")\n",
    "else:\n",
    "    y = T.ivector(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the vector of weights and the bias.\n",
    "There are num_classes weights associated to each feature, where the output is num_classes neurons.\n",
    "There are num_classes biases since there are num_classes output neurons.\n",
    "The weights are randomly initialised, the biases can be initialised to 0.0 or a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(num_classes < 3):\n",
    "    w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "    b = theano.shared(0.01, name=\"b\")\n",
    "else:\n",
    "    w = theano.shared(rng.randn(feats, num_classes), name=\"w\")\n",
    "    b = theano.shared(numpy.full(num_classes, 0.01), name=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional printing of the initial model weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Initial model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the actual solution. \n",
    "Sigma represent the sigmoid \n",
    "<br>\n",
    "$$ \\frac{1}{1+exp(-\\bf{x}\\dot\\bf{w}-\\bf{b}) } $$ \n",
    "<br>\n",
    "\n",
    "that is expressed in theano as T.nnet.sigmoid(). \n",
    "For a multi-class classification, sigma will be represented by a vector \n",
    "\n",
    "$$ \\sigma_{1}, \\dots, \\sigma_{j}, \\dots, \\sigma_{num\\_classes} $$ where\n",
    "\n",
    "<br>\n",
    "$$ \\sigma_{j} = \\frac{exp(\\bf{x}\\dot\\bf{w_{j}}+\\bf{b})}{\\sum_{i=1}^{num\\_classes}exp(\\bf{x}\\dot\\bf{w_{i}}+\\bf{b}) } $$\n",
    "<br>\n",
    "\n",
    "and the theano function representing it is called T.nnet.softmax().\n",
    "\n",
    "For a 2-class classification, it is enough to check whether sigma is greater than 0.5, otherwise we take \n",
    "The prediction can either be 0 or 1 (the two classes) depending on whether the sigmoid is greater or less than 0.5.\n",
    "The cost function is defined by \n",
    "\n",
    "<br>\n",
    "$$ \n",
    "error({\\bf w}) = -\\frac{1}{N} \\sum_{i=1}^{N} [ y^i \\ln (\\sigma({\\bf{x^i}})) + (1-y^i) \\ln (1 - \\sigma({\\bf{x^i}})] \n",
    "$$\n",
    "<br>\n",
    "\n",
    "where the superscript represents the $i^{th}$ example.\n",
    "For a multi-class classification, the cost function is modified to be:\n",
    "\n",
    "<br>\n",
    "$$ \n",
    "error({\\bf w}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{num\\_classes} \\{ y_{j}=j\\} [ y^i \\ln (\\sigma_{j}({\\bf{x^i}}))] \n",
    "$$\n",
    "<br>\n",
    "\n",
    "which in theano is defined by T.nnet.softmax(). The prediction will be the output class with thehighest value, i.e. the argmax of the output sigma.\n",
    "The cost adds a value to reduce the possibility of overfitting by keeping larger weights in check.\n",
    "Finally, theano will calculate the gradient of the cost function that is used for approximating the solution using linear descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct Theano expression graph\n",
    "#sigma = 1 / (1 + T.exp(-T.dot(x, w) - b))        # Probability that target = 1\n",
    "if(num_classes < 3):\n",
    "    sigma = T.nnet.sigmoid(T.dot(x,w) + b)        # The prediction thresholded\n",
    "    prediction = sigma > 0.5 \n",
    "    print \"Using two classes\"\n",
    "else:\n",
    "    sigma = T.nnet.softmax(T.dot(x,w) + b) \n",
    "    prediction = T.argmax(sigma, axis=1)          # The class with highest probability\n",
    "    print \"Using %i classes\"%num_classes\n",
    " \n",
    "# Cross-entropy loss function\n",
    "if( num_classes < 3):\n",
    "    xent = -y * T.log(sigma) - (1-y) * T.log(1-sigma) \n",
    "else:\n",
    "    xent = -T.mean(T.log(sigma)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum()        # Regularisation \n",
    "gw, gb = T.grad(cost, [w, b])                     # Compute the gradient of the cost         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We create the theano function.\n",
    "The input is given by the set of features per each example.\n",
    "The output is given by the class per each example.\n",
    "The training is performed by updating weights and biases using the gradient calculated times a training rate (in order to avoid overshooting the minimum value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[xent],\n",
    "          updates=((w, w - tr_rate * gw), (b, b - tr_rate * gb)),\n",
    "          allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[x], outputs=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the actual training on the data. \n",
    "This updates at each step the weighs and bias making the neural net perform better and get closer to the target solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "for i in range(training_steps):\n",
    "    train(Train[0], Train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional printing of the final model weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Final model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing of the target values (the classes) and the prediction by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"target values:\")\n",
    "print(Train[1])\n",
    "print(\"prediction:\")\n",
    "print(predict(Train[0]))\n",
    "\n",
    "if data == \"mnist\":\n",
    "    print(\"validation values:\")\n",
    "    print(Valid[1])\n",
    "    print(\"prediction:\")\n",
    "    print(predict(Valid[0]))\n",
    "\n",
    "    print(\"test values:\")\n",
    "    print(Test[1])\n",
    "    print(\"prediction:\")\n",
    "    print(predict(Test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the errors, i.e. the numbers of examples in the training set that have not been classified correctly and output the accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0\n",
    "\n",
    "print \"Results for Training set\"\n",
    "\n",
    "N = Train[1].shape[0]\n",
    "error = 0\n",
    "\n",
    "if N > 1000 : draw_images = False\n",
    "result = predict(Train[0])\n",
    "for index in range(N):\n",
    "    \n",
    "    act =  numpy.asscalar(Train[1][index])\n",
    "    prd =  numpy.asscalar(result[index])\n",
    "    if act != prd:\n",
    "        error += 1\n",
    "        print \"Predicted %i, actual value %i\" %(prd, act) \n",
    "        \n",
    "        if draw_images :\n",
    "            tmp = numpy.reshape(Train[0][index], [28, 28])\n",
    "            plt.figure()\n",
    "            plt.imshow(tmp, cmap = cm.Greys_r)\n",
    "        \n",
    "        \n",
    "correct_guesses = N - error\n",
    "accuracyTr = (N - error)*100./N\n",
    "\n",
    "print\n",
    "print \"correct predictions on training data = %i over %i examples\" % (correct_guesses, N)\n",
    "print \"accuracy on training data = %f%%\" % accuracyTr\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the errors, i.e. the numbers of examples in the validation set that have not been classified correctly and output the accuracy result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data == \"mnist\":\n",
    "    print \"Results for Validation set\"\n",
    "    \n",
    "    N = Valid[1].shape[0]  \n",
    "    error = 0\n",
    "    result = predict(Valid[0])\n",
    "    \n",
    "    for index in range(N):\n",
    "\n",
    "        act =  numpy.asscalar(Valid[1][index])\n",
    "        prd =  numpy.asscalar(result[index])\n",
    "        if act != prd:\n",
    "            error += 1\n",
    "            print \"Predicted %i, actual value %i\" %(prd, act) \n",
    "            if draw_images :\n",
    "                tmp = numpy.reshape(Valid[0][index], [28, 28])\n",
    "                plt.figure()\n",
    "                plt.imshow(tmp, cmap = cm.Greys_r)\n",
    "\n",
    "\n",
    "    correct_guesses = N - error\n",
    "    accuracyV = (N - error)*100./N\n",
    "\n",
    "    print\n",
    "    print \"correct predictions on validation data = %i over %i examples\" % (correct_guesses, N)\n",
    "    print \"accuracy on validation data = %f%%\" % accuracyV\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the errors, i.e. the numbers of examples in the testing set that have not been classified correctly and output the accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data == \"mnist\":\n",
    "    print \"Results for Testing set\"\n",
    "        \n",
    "    N = Test[1].shape[0] \n",
    "    error = 0\n",
    "    result = predict(Test[0])\n",
    "\n",
    "    for index in range(N):\n",
    "\n",
    "        act =  numpy.asscalar(Test[1][index])\n",
    "        prd =  numpy.asscalar(result[index])\n",
    "        if act != prd:\n",
    "            error += 1\n",
    "            print \"Predicted %i, actual value %i\" %(prd, act) \n",
    "            if draw_images :\n",
    "                tmp = numpy.reshape(Test[0][index], [28, 28])\n",
    "                plt.figure()\n",
    "                plt.imshow(tmp, cmap = cm.Greys_r)\n",
    "    \n",
    "    correct_guesses = N - error\n",
    "    accuracyTs = (N - error)*100./N\n",
    "\n",
    "    print\n",
    "    print \"correct predictions on testing data = %i over %i examples\" % (correct_guesses, N)\n",
    "    print \"accuracy on testing data = %f%%\" % accuracyTs\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print\n",
    "print \"accuracy on training data = %f%%\" % accuracyTr\n",
    "print\n",
    "\n",
    "if data == \"mnist\" :\n",
    "    print\n",
    "    print \"accuracy on validation data = %f%%\" % accuracyV\n",
    "    print\n",
    "\n",
    "    print\n",
    "    print \"accuracy on testing data = %f%%\" % accuracyTs\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
