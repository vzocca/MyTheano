{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple logistic regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "if( num_classes > 2):\n",
    "    multiclass = True\n",
    "else:\n",
    "    multiclass = False\n",
    "\n",
    "print \"Multiclass case is: \"    \n",
    "print multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of input neurons (the features), the number of examples, and the training rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#N = number of examples\n",
    "N = 1000\n",
    "#feats = number of input neurons\n",
    "feats = 784\n",
    "#training rate\n",
    "tr_rate = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a tensor with two entries:\n",
    "the first entry is a matrix of size N (number of examples) by feats (number of features) of random numbers on a normal distribution around 0.\n",
    "The second entry is a vector of size N (number of examples) of either 0 or 1 (the two classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=num_classes))\n",
    "training_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the symbolic variables.\n",
    "x will represent the input, i.e. a matrix of random numbers of size feats for each example (this is the D[0] entry defined above.\n",
    "y will represent the output, i.e. whether the example belongs to class 0 or class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "if(multiclass is False):\n",
    "    y = T.vector(\"y\")\n",
    "else:\n",
    "    y = T.ivector(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the vector of weights and the bias.\n",
    "There is just one weight associated to each feature since the output is just one class.\n",
    "There is just one bias since there is just one output neuron.\n",
    "The weights are randomly initialised, the bias can be initialised to 0.0 or a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(multiclass is False):\n",
    "    w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "    b = theano.shared(0.01, name=\"b\")\n",
    "else:\n",
    "    w = theano.shared(rng.randn(feats, num_classes), name=\"w\")\n",
    "    b = theano.shared(numpy.full(num_classes, 0.01), name=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional printing of the initial model weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Initial model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the actual solution. \n",
    "Sigma represent the sigmoid \n",
    "<br>\n",
    "$$ \\frac{1}{1+exp(-\\bf{x}\\dot\\bf{w}-\\bf{b}) } $$ \n",
    "<br>\n",
    "\n",
    "that is expressed in theano as T.nnet.sigmoid(). \n",
    "For a multi-class classification, sigma will be represented by a vector \n",
    "\n",
    "$$ \\sigma_{1}, \\dots, \\sigma_{j}, \\dots, \\sigma_{num\\_classes} $$ where\n",
    "\n",
    "<br>\n",
    "$$ \\sigma_{j} = \\frac{exp(\\bf{x}\\dot\\bf{w_{j}}+\\bf{b})}{\\sum_{i=1}^{num\\_classes}exp(\\bf{x}\\dot\\bf{w_{i}}+\\bf{b}) } $$\n",
    "<br>\n",
    "\n",
    "and the theano function representing it is called T.nnet.softmax().\n",
    "\n",
    "For a 2-class classification, it is enough to check whether sigma is greater than 0.5, otherwise we take \n",
    "The prediction can either be 0 or 1 (the two classes) depending on whether the sigmoid is greater or less than 0.5.\n",
    "The cost function is defined by \n",
    "\n",
    "<br>\n",
    "$$ \n",
    "error({\\bf w}) = -\\frac{1}{N} \\sum_{i=1}^{N} [ y^i \\ln (\\sigma({\\bf{x^i}})) + (1-y^i) \\ln (1 - \\sigma({\\bf{x^i}})] \n",
    "$$\n",
    "<br>\n",
    "\n",
    "where the superscript represents the $i^{th}$ example.\n",
    "For a multi-class classification, the cost function is modified to be:\n",
    "\n",
    "<br>\n",
    "$$ \n",
    "error({\\bf w}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{num\\_classes} \\{ y_{j}=j\\} [ y^i \\ln (\\sigma_{j}({\\bf{x^i}}))] \n",
    "$$\n",
    "<br>\n",
    "\n",
    "which in theano is defined by T.nnet.softmax(). The prediction will be the output class with thehighest value, i.e. the argmax of the output sigma.\n",
    "The cost adds a value to reduce the possibility of overfitting by keeping larger weights in check.\n",
    "Finally, theano will calculate the gradient of the cost function that is used for approximating the solution using linear descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct Theano expression graph\n",
    "#sigma = 1 / (1 + T.exp(-T.dot(x, w) - b))        # Probability that target = 1\n",
    "if(multiclass is False):\n",
    "    sigma = T.nnet.sigmoid(T.dot(x,w) + b)        # The prediction thresholded\n",
    "    prediction = sigma > 0.5 \n",
    "    print \"Two classes\"\n",
    "else:\n",
    "    sigma = T.nnet.softmax(T.dot(x,w) + b) \n",
    "    prediction = T.argmax(sigma, axis=1)          # The class with highest probability\n",
    "    print \"%i classes\"%num_classes\n",
    " \n",
    "# Cross-entropy loss function\n",
    "if( multiclass is False):\n",
    "    xent = -y * T.log(sigma) - (1-y) * T.log(1-sigma) \n",
    "else:\n",
    "    xent = -T.mean(T.log(sigma)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "cost = xent + 0.01 * (w ** 2).sum()               # Regularisation \n",
    "gw, gb = T.grad(cost, [w, b])                     # Compute the gradient of the cost         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We create the theano function.\n",
    "The input is given by the set of features per each example.\n",
    "The output is given by the class per each example.\n",
    "The training is performed by updating weights and biases using the gradient calculated times a training rate (in order to avoid overshooting the minimum value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[xent],\n",
    "          updates=((w, w - tr_rate * gw), (b, b - tr_rate * gb)),\n",
    "          allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[x], outputs=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the actual training on the data. \n",
    "This updates at each step the weighs and bias making the neural net perform better and get closer to the target solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "for i in range(training_steps):\n",
    "    train(D[0], D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional printing of the final model weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Final model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing of the target values (the classes) and the prediction by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"target values for D:\")\n",
    "print(D[1])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(D[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the errors, i.e. the numbers of examples that have not been classified correctly and output the accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = predict(D[0]) - D[1]\n",
    "\n",
    "error = 0\n",
    "for index in result:\n",
    "    if result[index] != 0:\n",
    "        error += 1\n",
    "        \n",
    "correct_guesses = N - error\n",
    "accuracy = (N - error)*100/N\n",
    "\n",
    "print\n",
    "print \"correct predictions = %f over %i examples\" % (correct_guesses, N)\n",
    "print \"accuracy = %i%%\" % accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
